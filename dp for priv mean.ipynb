{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75abefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algs_lib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd95edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_threshold(vec, c):\n",
    "    curr_norm = np.linalg.norm(vec)\n",
    "#     print(curr_norm, c)\n",
    "    if curr_norm <= c:\n",
    "        return vec\n",
    "#     print('reached')\n",
    "    clip_ratio = c / curr_norm\n",
    "    return [vec[i]*clip_ratio for i in range(len(vec))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82529d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(scale):\n",
    "    return np.random.laplace(0, scale)\n",
    "# global sensitivity is C/n i think?\n",
    "# so scale should be (C/n) / \\epsilon per elem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94210938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_posterior(mi, prior=0.5, prec = 100000):\n",
    "    test_vals = [x / prec for x in range(1, prec)]\n",
    "    max_t = None\n",
    "    for t in test_vals:\n",
    "        if t*np.log(t/prior)+(1-t)*np.log((1-t)/(1-prior)) <= mi:\n",
    "            if  max_t is None or t > max_t:\n",
    "                max_t = t\n",
    "    return max_t\n",
    "\n",
    "def dp_epsilon_to_posterior_success(epsilon):\n",
    "    return 1 - 1./(1+np.exp(epsilon))\n",
    "\n",
    "def dp_ps_to_epsilon(ps):\n",
    "    return np.log(ps / (1-ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8b3307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_posterior(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9873d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_noise_mean(train_x, train_y, subsample_rate, num_classes,\n",
    "    eta, regularize=None, num_trees=None, tree_depth = None, max_mi = 0.5, num_dims = None):\n",
    "\n",
    "    sec_v = max_mi / 2\n",
    "    sec_beta = max_mi - sec_v\n",
    "    r = calc_r(train_x)\n",
    "    gamma = 0.01\n",
    "    avg_dist = 0.\n",
    "    curr_est = None\n",
    "    converged = False\n",
    "    curr_trial = 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = len(set(train_y))\n",
    "\n",
    "    assert subsample_rate >= num_classes\n",
    "\n",
    "    est_y = {}\n",
    "    prev_ests = None\n",
    "    # 10*c*v\n",
    "    seed = np.random.randint(1, 100000)\n",
    "\n",
    "    s1 = None # only relevant for PCA\n",
    "    while not converged:\n",
    "        shuffled_x, shuffled_y = shuffle(train_x, train_y)\n",
    "        \n",
    "        shuffled_x, shuffled_y = get_samples_safe(shuffled_x, shuffled_y, num_classes, subsample_rate)\n",
    "        \n",
    "        output = np.average(shuffled_x, axis=0)\n",
    "\n",
    "        for ind in range(len(output)):\n",
    "            if ind not in est_y:\n",
    "                est_y[ind] = []\n",
    "            est_y[ind].append(output[ind])\n",
    "\n",
    "        if curr_trial % 10 == 0:\n",
    "            if curr_trial % 100 == 0:\n",
    "                print(f'curr trial is {curr_trial}')\n",
    "            if prev_ests is None:\n",
    "                prev_ests = {}\n",
    "                for ind in est_y:\n",
    "                    prev_ests[ind] = np.var(est_y[ind])\n",
    "            else:\n",
    "                converged = True\n",
    "                for ind in est_y:\n",
    "                    if abs(np.var(est_y[ind]) - prev_ests[ind]) > eta:\n",
    "                        converged = False\n",
    "                if not converged:\n",
    "                    for ind in est_y:\n",
    "                        prev_ests[ind] = np.var(est_y[ind])\n",
    "        curr_trial += 1\n",
    "    fin_var = {ind: np.var(est_y[ind]) for ind in est_y}\n",
    "\n",
    "    noise = {}\n",
    "    sqrt_total_var = sum([fin_var[x]**0.5 for x in fin_var])\n",
    "    for ind in fin_var:\n",
    "        noise[ind] = 1./(2*max_mi) * fin_var[ind]**0.5 * sqrt_total_var\n",
    "    return noise\n",
    "\n",
    "\n",
    "def hybrid_noise_auto_ind(train_x, train_y, subsample_rate, num_classes,\n",
    "    eta, regularize=None, num_trees=None, tree_depth = None, max_mi = 0.5, num_dims = None):\n",
    "    curr_est = None\n",
    "    converged = False\n",
    "    curr_trial = 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = len(set(train_y))\n",
    "\n",
    "    assert subsample_rate >= num_classes\n",
    "\n",
    "    est_y = {}\n",
    "    prev_ests = None\n",
    "\n",
    "    s1 = None # only relevant for PCA\n",
    "    \n",
    "    max_noises = {}\n",
    "    \n",
    "    for ind in range(len(train_x)):\n",
    "        print(f\"ind = {ind}\")\n",
    "        removed_train_x = np.delete(train_x, ind, 0)\n",
    "        removed_train_y = np.delete(train_y, ind, 0)\n",
    "        while not converged:\n",
    "            shuffled_x, shuffled_y = shuffle(removed_train_x, removed_train_y)\n",
    "\n",
    "            shuffled_x, shuffled_y = get_samples_safe(shuffled_x, shuffled_y, num_classes, subsample_rate)\n",
    "            \n",
    "            added_x = copy.deepcopy(shuffled_x)\n",
    "            added_y = copy.deepcopy(shuffled_y)\n",
    "            added_x[0] = train_x[ind]\n",
    "            added_y[0] = train_y[ind]\n",
    "\n",
    "            output_orig = np.average(shuffled_x, axis=0)\n",
    "            output_new = np.average(added_x, axis=0)\n",
    "            output = (output_orig - output_new)**2\n",
    "\n",
    "            for ind in range(len(output)):\n",
    "                if ind not in est_y:\n",
    "                    est_y[ind] = []\n",
    "                est_y[ind].append(output[ind])\n",
    "\n",
    "            if curr_trial % 10 == 0:        \n",
    "                if prev_ests is None:\n",
    "                    prev_ests = {}\n",
    "                    for ind in est_y:\n",
    "                        prev_ests[ind] = np.var(est_y[ind])\n",
    "                else:\n",
    "                    converged = True\n",
    "                    for ind in est_y:\n",
    "                        if abs(np.var(est_y[ind]) - prev_ests[ind]) > eta:\n",
    "                            converged = False\n",
    "                    if not converged:\n",
    "                        for ind in est_y:\n",
    "                            prev_ests[ind] = np.var(est_y[ind])\n",
    "            curr_trial += 1\n",
    "        fin_var = {ind: np.var(est_y[ind]) for ind in est_y}\n",
    "\n",
    "        noise = {}\n",
    "        sqrt_total_var = sum([fin_var[x]**0.5 for x in fin_var])\n",
    "        for ind in fin_var:\n",
    "            noise[ind] = 1./(2*max_mi) * fin_var[ind]**0.5 * sqrt_total_var\n",
    "        for ind in noise:\n",
    "            if ind not in max_noises or max_noises[ind] < noise[ind]:\n",
    "                max_noises[ind] = noise[ind]\n",
    "    return max_noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b3dbeba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41666667 0.33333333 0.69491525 0.95833333]\n",
      " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.83333333 0.375      0.89830508 0.70833333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]]\n",
      "[[0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.83333333 0.375      0.89830508 0.70833333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x[:5]\n",
    "print(train_x)\n",
    "print(np.delete(train_x, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b3e34fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6853970684247657\n",
      "{0: 0.002289270346102352, 1: 0.0018726066562047249, 2: 0.0030400308880141748, 3: 0.0032905573978312327}\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_iris(normalize=True)\n",
    "\n",
    "norms = [np.linalg.norm(x) for x in train_x]\n",
    "orig_noise = hybrid_noise_mean(train_x, train_y, subsample_rate, num_classes, 1e-6)\n",
    "\n",
    "print(max(norms))\n",
    "print(orig_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e5d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6426117097961406, 0.7304317044395013, 0.3563228120191924]\n",
      "[0.83789, 0.6749, 0.58815]\n"
     ]
    }
   ],
   "source": [
    "true_mean = np.average(train_x, axis=0)\n",
    "\n",
    "mi_range = [0.25, 1/16., 0.015625]\n",
    "posterior_success_rates = [calc_posterior(mi) for mi in mi_range]\n",
    "epsilon_vals = [dp_ps_to_epsilon(ps) for ps in posterior_success_rates]\n",
    "\n",
    "print(epsilon_vals)\n",
    "print([x for x in posterior_success_rates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8dac0bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "14\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# DP MEAN\n",
    "dp_dists = {}\n",
    "num_trials = 1000\n",
    "for eps in epsilon_vals:\n",
    "    avg_dist_dp = {}\n",
    "    for i in range(1, 17):\n",
    "        clip_budget = i / 10\n",
    "        clipped_train_x = [clip_to_threshold(train_x[i], clip_budget) for i in range(len(train_x))]\n",
    "        released_mean = np.average(clipped_train_x, axis=0)\n",
    "        clip_dist = np.linalg.norm(released_mean - true_mean)\n",
    "        dist = 0.\n",
    "        sensitivity = clip_budget / train_len\n",
    "        for _ in range(num_trials):\n",
    "            released_mean = np.average(clipped_train_x, axis=0)\n",
    "            for ind in range(len(released_mean)):\n",
    "                sensitivity = clip_budget / train_len\n",
    "                released_mean[ind] += add_noise(sensitivity/eps)\n",
    "            dist += np.linalg.norm(released_mean - true_mean)\n",
    "        dist /= num_trials\n",
    "        avg_dist_dp[i] = (clip_dist, dist)\n",
    "    dp_key = min(avg_dist_dp.items(), key=lambda x: x[1][1])[0]\n",
    "    print(dp_key)\n",
    "    dp_dists[eps] = avg_dist_dp[dp_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1de7c6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.6426117097961406: (0.003760138063449314, 0.023091294506178354),\n",
       " 0.7304317044395013: (0.01164606776365974, 0.04880799508990495),\n",
       " 0.3563228120191924: (0.05011494917674307, 0.0956881524433286)}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b8332eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1429e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampled_dist = 0.04369795223162045\n",
      "0.044892926450963024\n",
      "-----\n",
      "subsampled_dist = 0.043053740909081324\n",
      "0.05633762085921197\n",
      "-----\n",
      "subsampled_dist = 0.043495379785967965\n",
      "0.15442332129575034\n",
      "-----\n",
      "{0.25: (0.04369795223162045, 0.044892926450963024, 0.04663600714844551), 0.0625: (0.043053740909081324, 0.05633762085921197, 0.06318522972963278), 0.015625: (0.043495379785967965, 0.15442332129575034, 0.19137743177062086)}\n"
     ]
    }
   ],
   "source": [
    "# PAC MEAN\n",
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_iris(normalize=True)\n",
    "true_mean = np.average(train_x, axis=0)\n",
    "\n",
    "\n",
    "norms = [np.linalg.norm(x) for x in train_x]\n",
    "# print(max(norms))\n",
    "\n",
    "subsample_rate = int(0.5*train_len)\n",
    "\n",
    "pac_dists = {}\n",
    "num_trials = 1000\n",
    "for mi in mi_range:\n",
    "    scaled_noise = {k: noise[k] * (0.5 / mi) for k in noise}\n",
    "    iso_noise = max(scaled_noise.values())\n",
    "    iso_scaled = {k: iso_noise for k in noise}\n",
    "    avg_dist_pac = 0\n",
    "    avg_iso_dist_pac = 0\n",
    "    subsampled_dist = 0\n",
    "    noise = hybrid_noise_auto(train_x, train_y, subsample_rate, num_classes, 1e-6)\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        subsampled_dist += np.linalg.norm(released_mean - true_mean)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=scaled_noise[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    subsampled_dist /= num_trials\n",
    "    print(f'subsampled_dist = {subsampled_dist}')\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=iso_scaled[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_iso_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    \n",
    "    avg_iso_dist_pac /= num_trials\n",
    "    avg_dist_pac /= num_trials\n",
    "    print(avg_dist_pac)\n",
    "    print('-----')\n",
    "#     subsampled_dist /= num_trials\n",
    "    pac_dists[mi] = (subsampled_dist, avg_dist_pac, avg_iso_dist_pac)\n",
    "print(pac_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6eaa2ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.25: (0.04369795223162045, 0.044892926450963024, 0.04663600714844551),\n",
       " 0.0625: (0.043053740909081324, 0.05633762085921197, 0.06318522972963278),\n",
       " 0.015625: (0.043495379785967965, 0.15442332129575034, 0.19137743177062086)}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559fd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_bean(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d933e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0829053788327507\n"
     ]
    }
   ],
   "source": [
    "true_mean = np.average(train_x, axis=0)\n",
    "\n",
    "norms = [np.linalg.norm(x) for x in train_x]\n",
    "print(max(norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb0127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14015323, 0.22728516, 0.24730455, 0.23645427, 0.39973723,\n",
       "       0.76977418, 0.13719586, 0.22584614, 0.62437111, 0.90016561,\n",
       "       0.7647052 , 0.45709353, 0.49354762, 0.36943584, 0.41102562,\n",
       "       0.91006917])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc5c195",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trials):\n\u001b[0;32m---> 14\u001b[0m     released_mean \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_train_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(released_mean)):\n\u001b[1;32m     16\u001b[0m         sensitivity \u001b[38;5;241m=\u001b[39m clip_budget \u001b[38;5;241m/\u001b[39m train_len \n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/numpy/lib/function_base.py:509\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_average_dispatcher)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, returned\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    398\u001b[0m             keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Compute the weighted average along the specified axis.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m           [4.5]])\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue:\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;66;03m# Don't pass on the keepdims argument if one wasn't given.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m         keepdims_kw \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DP MEAN\n",
    "dp_dists = {}\n",
    "num_trials = 1000\n",
    "\n",
    "for eps in epsilon_vals:\n",
    "    avg_dist_dp = {}\n",
    "    for i in range(1, 31):\n",
    "        clip_budget = i/10\n",
    "        clipped_train_x = [clip_to_threshold(train_x[i], clip_budget) for i in range(len(train_x))]\n",
    "        released_mean = np.average(clipped_train_x, axis=0)\n",
    "        clip_dist = np.linalg.norm(released_mean - true_mean)\n",
    "        dist = 0.\n",
    "        for _ in range(num_trials):\n",
    "            released_mean = np.average(clipped_train_x, axis=0)\n",
    "            for ind in range(len(released_mean)):\n",
    "                sensitivity = clip_budget / train_len \n",
    "                released_mean[ind] += add_noise(sensitivity / eps)\n",
    "            dist += np.linalg.norm(released_mean - true_mean)\n",
    "        dist /= num_trials\n",
    "        avg_dist_dp[i] = (clip_dist, dist)\n",
    "    dp_key = min(avg_dist_dp.items(), key=lambda x: x[1][1])[0]\n",
    "    dp_dists[eps] = avg_dist_dp[dp_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9188f447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.6426117097961406: (6.585062837225667e-05, 0.0010065313905155784),\n",
       " 0.7304317044395013: (0.0002614561788240281, 0.0021905352978012884),\n",
       " 0.3563228120191924: (0.000942316538515316, 0.004439134273849168)}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8569732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0019812757614805806, 0.002553987238147606),\n",
       " (0.0019812757614805806, 0.00399900130072505),\n",
       " (0.0019812757614805806, 0.007624172088701473)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dp_dists[x] for x in dp_dists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd34b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr trial is 0\n",
      "0.0005653131897607153\n"
     ]
    }
   ],
   "source": [
    "# PAC MEAN\n",
    "\n",
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_bean(normalize=True)\n",
    "true_mean = np.average(train_x, axis=0)\n",
    "\n",
    "\n",
    "subsample_rate = int(0.5*train_len)\n",
    "\n",
    "noise = hybrid_noise_mean(train_x, train_y, subsample_rate, num_classes, 1e-6)\n",
    "print(sum(noise.values()))\n",
    "pac_dists = {}\n",
    "num_trials = 1000\n",
    "\n",
    "for mi in mi_range:\n",
    "    scaled_noise = {k: noise[k] * (0.5 / mi) for k in noise}\n",
    "    iso_noise = max(scaled_noise.values())\n",
    "    iso_scaled = {k: iso_noise for k in noise}\n",
    "    avg_dist_pac = 0\n",
    "    avg_iso_dist_pac = 0\n",
    "    subsampled_dist = 0\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        subsampled_dist += np.linalg.norm(released_mean - true_mean)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=scaled_noise[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=iso_scaled[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_iso_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    avg_iso_dist_pac /= num_trials\n",
    "    avg_dist_pac /= num_trials\n",
    "    subsampled_dist /= num_trials\n",
    "    pac_dists[mi] = (subsampled_dist, avg_dist_pac, avg_iso_dist_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5ab9f416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.25: (0.005492895735124468, 0.005504368899810956, 0.005600894228118885),\n",
       " 0.0625: (0.00539451708927943, 0.005548361536118546, 0.005655970716286265),\n",
       " 0.015625: (0.005417806532302793, 0.007328477886119223, 0.00892565593821365)}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c55462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PAC MEAN\n",
    "# subsample_rate = int(0.5*train_len)\n",
    "\n",
    "# noise = hybrid_noise_auto_ind(train_x, train_y, subsample_rate, num_classes, 1e-6)\n",
    "\n",
    "# pac_dists = {}\n",
    "# num_trials = 1000\n",
    "\n",
    "# for mi in mi_range:\n",
    "#     scaled_noise = {k: noise[k] * (0.5 / mi) for k in noise}\n",
    "#     iso_noise = max(scaled_noise.values())\n",
    "#     iso_scaled = {k: iso_noise for k in noise}\n",
    "#     avg_dist_pac = 0\n",
    "#     avg_iso_dist_pac = 0\n",
    "#     subsampled_dist = 0\n",
    "#     for _ in range(num_trials):\n",
    "#         shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "#         shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "#         released_mean = np.average(shuffled_x1, axis=0)\n",
    "#         subsampled_dist += np.linalg.norm(released_mean - true_mean)\n",
    "#         for ind in range(len(released_mean)):\n",
    "#             c = np.random.normal(0, scale=scaled_noise[ind])\n",
    "#             released_mean[ind] += c\n",
    "#         avg_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "#     for _ in range(num_trials):\n",
    "#         shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "#         shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "#         released_mean = np.average(shuffled_x1, axis=0)\n",
    "#         for ind in range(len(released_mean)):\n",
    "#             c = np.random.normal(0, scale=iso_scaled[ind])\n",
    "#             released_mean[ind] += c\n",
    "#         avg_iso_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "#     avg_iso_dist_pac /= num_trials\n",
    "#     avg_dist_pac /= num_trials\n",
    "#     subsampled_dist /= num_trials\n",
    "#     pac_dists[mi] = (subsampled_dist, avg_dist_pac, avg_iso_dist_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0426f57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.25: (0.005492895735124468, 0.005504368899810956, 0.005600894228118885),\n",
       " 0.0625: (0.00539451708927943, 0.005548361536118546, 0.005655970716286265),\n",
       " 0.015625: (0.005417806532302793, 0.007328477886119223, 0.00892565593821365)}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f95fbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7351afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_rice(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "963bb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.399557199470374\n"
     ]
    }
   ],
   "source": [
    "true_mean = np.average(train_x, axis=0)\n",
    "\n",
    "norms = [np.linalg.norm(x) for x in train_x]\n",
    "print(max(norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "914db068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45223512, 0.50394861, 0.46521187, 0.5597932 , 0.64152809,\n",
       "       0.46161908, 0.45263566])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d11e9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP MEAN\n",
    "dp_dists = {}\n",
    "num_trials = 1000\n",
    "\n",
    "for eps in epsilon_vals:\n",
    "    avg_dist_dp = {}\n",
    "    for i in range(1, 3):\n",
    "        clip_budget = i\n",
    "        clipped_train_x = [clip_to_threshold(train_x[i], clip_budget) for i in range(len(train_x))]\n",
    "        released_mean = np.average(clipped_train_x, axis=0)\n",
    "        clip_dist = np.linalg.norm(released_mean - true_mean)\n",
    "        dist = 0.\n",
    "        for _ in range(num_trials):\n",
    "            released_mean = np.average(clipped_train_x, axis=0)\n",
    "            for ind in range(len(released_mean)):\n",
    "                sensitivity = clip_budget / train_len \n",
    "                released_mean[ind] += add_noise(sensitivity / eps)\n",
    "            dist += np.linalg.norm(released_mean - true_mean)\n",
    "        dist /= num_trials\n",
    "        avg_dist_dp[i] = (clip_dist, dist)\n",
    "    dp_key = min(avg_dist_dp.items(), key=lambda x: x[1][1])[0]\n",
    "    dp_dists[eps] = avg_dist_dp[dp_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f84fb3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.6426117097961406: (0.0016312321401344787, 0.002292359980503194),\n",
       " 0.7304317044395013: (0.0016312321401344787, 0.0038929025067448103),\n",
       " 0.3563228120191924: (0.0016312321401344787, 0.007434481194003592)}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7e8af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAC MEAN\n",
    "\n",
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_rice(normalize=True)\n",
    "true_mean = np.average(train_x, axis=0)\n",
    "subsample_rate = int(0.5*train_len)\n",
    "\n",
    "\n",
    "noise = hybrid_noise_auto(train_x, train_y, subsample_rate, num_classes, 1e-6)\n",
    "\n",
    "pac_dists = {}\n",
    "num_trials = 1000\n",
    "\n",
    "for mi in mi_range:\n",
    "    scaled_noise = {k: noise[k] * (0.5 / mi) for k in noise}\n",
    "    iso_noise = max(scaled_noise.values())\n",
    "    iso_scaled = {k: iso_noise for k in noise}\n",
    "    avg_dist_pac = 0\n",
    "    avg_iso_dist_pac = 0\n",
    "    subsampled_dist = 0\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        subsampled_dist += np.linalg.norm(released_mean - true_mean)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=scaled_noise[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=iso_scaled[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_iso_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    avg_iso_dist_pac /= num_trials\n",
    "    avg_dist_pac /= num_trials\n",
    "    subsampled_dist /= num_trials\n",
    "    pac_dists[mi] = (subsampled_dist, avg_dist_pac, avg_iso_dist_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c73c4799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.25: (0.007619588243238379, 0.007626673663862336, 0.007452167877120045),\n",
       " 0.0625: (0.007601176162287781, 0.007683088037882145, 0.00797367638558806),\n",
       " 0.015625: (0.0075220431904354824, 0.008809859813561569, 0.010569859683508558)}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b7ab42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "23f77a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cifar10(normalize=False):\n",
    "    fnames = ['cifar-10-batches-py/data_batch_{}'.format(i) for i in range(1, 6)]\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    for f in fnames:\n",
    "        data = unpickle(f)\n",
    "        all_x.extend(data[b'data'])\n",
    "        all_y.extend(data[b'labels'])\n",
    "    test_data = 'cifar-10-batches-py/test_batch'\n",
    "    test_data = unpickle(f)\n",
    "    all_x.extend(data[b'data'])\n",
    "    all_y.extend(data[b'labels'])\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        scaled = min_max_scaler.fit_transform(all_x)\n",
    "        all_x = np.array(pd.DataFrame(scaled))\n",
    "#     print(all_x.shape)\n",
    "    train_x = np.array(all_x[:50000, :])\n",
    "    test_x = np.array(all_x[50000:, :])\n",
    "#     print(train_x.shape)\n",
    "#     print(test_x.shape)\n",
    "    \n",
    "    train_y = np.array(all_y[:50000])\n",
    "    test_y = np.array(all_y[50000:])\n",
    "\n",
    "    num_classes = 10\n",
    "    train_len = train_x.shape[0]\n",
    "    return train_x, train_y, test_x, test_y, num_classes, train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ac074efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr trial is 0\n"
     ]
    }
   ],
   "source": [
    "# PAC MEAN\n",
    "\n",
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_cifar10(normalize=True)\n",
    "true_mean = np.average(train_x, axis=0)\n",
    "subsample_rate = int(0.5*train_len)\n",
    "\n",
    "\n",
    "noise = hybrid_noise_auto(train_x, train_y, subsample_rate, num_classes, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4bdf5bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.004188356421119193,\n",
       " 1: 0.0043902587621660666,\n",
       " 2: 0.004218196109612588,\n",
       " 3: 0.00391120058172516,\n",
       " 4: 0.0037325421246164835,\n",
       " 5: 0.0036269196160360773,\n",
       " 6: 0.0036615215194997905,\n",
       " 7: 0.0036336067956982505,\n",
       " 8: 0.003895481563404101,\n",
       " 9: 0.004264602520775066,\n",
       " 10: 0.0048089583283891756,\n",
       " 11: 0.0049650615989707955,\n",
       " 12: 0.0047066052069609,\n",
       " 13: 0.0043873121728876694,\n",
       " 14: 0.00401422022060526,\n",
       " 15: 0.0035170051855170563,\n",
       " 16: 0.002991171607874488,\n",
       " 17: 0.003159592404017243,\n",
       " 18: 0.003657950493853478,\n",
       " 19: 0.0037027455291419025,\n",
       " 20: 0.003694316073226612,\n",
       " 21: 0.003955270198976654,\n",
       " 22: 0.004203521653369557,\n",
       " 23: 0.003711044939899409,\n",
       " 24: 0.003442565571382252,\n",
       " 25: 0.003524050975364102,\n",
       " 26: 0.0033617636616469545,\n",
       " 27: 0.0035357050628646984,\n",
       " 28: 0.003310676656335339,\n",
       " 29: 0.0032482624245607316,\n",
       " 30: 0.003437511218460746,\n",
       " 31: 0.003254317548694189,\n",
       " 32: 0.00414210927920843,\n",
       " 33: 0.004262415461265851,\n",
       " 34: 0.004328415411320247,\n",
       " 35: 0.004014372894272318,\n",
       " 36: 0.003624240403452663,\n",
       " 37: 0.0039219802992725514,\n",
       " 38: 0.004334144894668542,\n",
       " 39: 0.004260960565209856,\n",
       " 40: 0.0044382585600584315,\n",
       " 41: 0.004283538618905986,\n",
       " 42: 0.004520766173670363,\n",
       " 43: 0.004961660250956762,\n",
       " 44: 0.0047750094961209605,\n",
       " 45: 0.004193683694460102,\n",
       " 46: 0.0036375768884978066,\n",
       " 47: 0.003219244260698135,\n",
       " 48: 0.002896689767792297,\n",
       " 49: 0.0032104493037196573,\n",
       " 50: 0.003347653595405551,\n",
       " 51: 0.0035980751295161045,\n",
       " 52: 0.0035759399587349467,\n",
       " 53: 0.0036281378314702824,\n",
       " 54: 0.0037190202257962324,\n",
       " 55: 0.003443500928428194,\n",
       " 56: 0.003517952474339737,\n",
       " 57: 0.003533262591656377,\n",
       " 58: 0.003329971188344123,\n",
       " 59: 0.0034182620455310603,\n",
       " 60: 0.003210985491419521,\n",
       " 61: 0.003045588457955989,\n",
       " 62: 0.0031352378872075712,\n",
       " 63: 0.0029682404059299897,\n",
       " 64: 0.0038509831251624764,\n",
       " 65: 0.0041621892357725804,\n",
       " 66: 0.004158317404232937,\n",
       " 67: 0.003894442388851668,\n",
       " 68: 0.003763060072851568,\n",
       " 69: 0.0037729953699535057,\n",
       " 70: 0.004450023731772502,\n",
       " 71: 0.004499924980864763,\n",
       " 72: 0.004446965474829747,\n",
       " 73: 0.004006095745695002,\n",
       " 74: 0.0038861761021791808,\n",
       " 75: 0.004015383415700413,\n",
       " 76: 0.003951285840562975,\n",
       " 77: 0.0038516161295925285,\n",
       " 78: 0.003881296652773214,\n",
       " 79: 0.0034809184460036255,\n",
       " 80: 0.0034217379764047273,\n",
       " 81: 0.0033041064763636354,\n",
       " 82: 0.003096829041591618,\n",
       " 83: 0.0035649675548510814,\n",
       " 84: 0.0037212785645063685,\n",
       " 85: 0.003531447797924638,\n",
       " 86: 0.003537023825974324,\n",
       " 87: 0.003417070660585184,\n",
       " 88: 0.003228807686898224,\n",
       " 89: 0.003387980041566419,\n",
       " 90: 0.003282842309134272,\n",
       " 91: 0.0034288618281000127,\n",
       " 92: 0.0034785535713689362,\n",
       " 93: 0.0032926937846293317,\n",
       " 94: 0.002997837920042283,\n",
       " 95: 0.0028587900709237824,\n",
       " 96: 0.0034627834204336372,\n",
       " 97: 0.004003542029955877,\n",
       " 98: 0.003804765930012642,\n",
       " 99: 0.0035732939911644014,\n",
       " 100: 0.003547653782301154,\n",
       " 101: 0.0035475289599106656,\n",
       " 102: 0.004504782876642926,\n",
       " 103: 0.0045000489745817135,\n",
       " 104: 0.0038281032319835073,\n",
       " 105: 0.00368325286919197,\n",
       " 106: 0.003819230052318202,\n",
       " 107: 0.0035542190384130466,\n",
       " 108: 0.0035437444985798395,\n",
       " 109: 0.0037642777161930167,\n",
       " 110: 0.004013657651470048,\n",
       " 111: 0.003838797468422357,\n",
       " 112: 0.003430555339119413,\n",
       " 113: 0.003295040077283977,\n",
       " 114: 0.0035173186371894663,\n",
       " 115: 0.0034942034391802776,\n",
       " 116: 0.0036128548947374347,\n",
       " 117: 0.0031290279226132093,\n",
       " 118: 0.0028848639505708916,\n",
       " 119: 0.0028753197514937434,\n",
       " 120: 0.002716847692718307,\n",
       " 121: 0.0028040302298112667,\n",
       " 122: 0.0029792824941667364,\n",
       " 123: 0.003591167089228597,\n",
       " 124: 0.0040314648106508835,\n",
       " 125: 0.0037331102734261415,\n",
       " 126: 0.0031531226602414136,\n",
       " 127: 0.0027956723549981636,\n",
       " 128: 0.0033530872125414253,\n",
       " 129: 0.0038597658835878928,\n",
       " 130: 0.003510118050075106,\n",
       " 131: 0.00318759249556932,\n",
       " 132: 0.003053160276337402,\n",
       " 133: 0.003255154478266137,\n",
       " 134: 0.004225795796420673,\n",
       " 135: 0.003858933738007204,\n",
       " 136: 0.0030992078951622686,\n",
       " 137: 0.00348504558204209,\n",
       " 138: 0.003748905008404748,\n",
       " 139: 0.0034504331695103144,\n",
       " 140: 0.003263360649903984,\n",
       " 141: 0.0035622611585948974,\n",
       " 142: 0.003697610824055704,\n",
       " 143: 0.003537078763765202,\n",
       " 144: 0.0034230458008752663,\n",
       " 145: 0.0034178866967169755,\n",
       " 146: 0.0035198275123207714,\n",
       " 147: 0.0030051698027649077,\n",
       " 148: 0.0031623901212577046,\n",
       " 149: 0.002947375865214484,\n",
       " 150: 0.0026002909283284314,\n",
       " 151: 0.002494803072501919,\n",
       " 152: 0.002569832199322529,\n",
       " 153: 0.002477657940522661,\n",
       " 154: 0.0029764321139675097,\n",
       " 155: 0.0036753187419087097,\n",
       " 156: 0.003600704487372748,\n",
       " 157: 0.003224541443555019,\n",
       " 158: 0.002822947999361962,\n",
       " 159: 0.00279190052730961,\n",
       " 160: 0.003153758854096144,\n",
       " 161: 0.0035601602848043495,\n",
       " 162: 0.003499112001040968,\n",
       " 163: 0.0033163851343460376,\n",
       " 164: 0.002883130884476063,\n",
       " 165: 0.003089378368604672,\n",
       " 166: 0.00382267088637816,\n",
       " 167: 0.00364690744000854,\n",
       " 168: 0.0032873287971175425,\n",
       " 169: 0.003583421370140357,\n",
       " 170: 0.0035268556163460018,\n",
       " 171: 0.0031854911541435963,\n",
       " 172: 0.0029383519862740113,\n",
       " 173: 0.0036053270724846313,\n",
       " 174: 0.0038135491991729453,\n",
       " 175: 0.0032601707632685527,\n",
       " 176: 0.003418082215224862,\n",
       " 177: 0.003445736175522215,\n",
       " 178: 0.003180586900557948,\n",
       " 179: 0.0029689724644860163,\n",
       " 180: 0.002910670223577452,\n",
       " 181: 0.0028882230217548844,\n",
       " 182: 0.003037469326734274,\n",
       " 183: 0.00257276792062463,\n",
       " 184: 0.002525439549705951,\n",
       " 185: 0.0029761811722703955,\n",
       " 186: 0.0029990162041360034,\n",
       " 187: 0.0035166229646160603,\n",
       " 188: 0.0034754430972608152,\n",
       " 189: 0.0029753657060640143,\n",
       " 190: 0.002849444404606233,\n",
       " 191: 0.0029851192502591967,\n",
       " 192: 0.002668103213890267,\n",
       " 193: 0.0028334215331526735,\n",
       " 194: 0.0029320171766669804,\n",
       " 195: 0.0030265673287111863,\n",
       " 196: 0.0029331494879996835,\n",
       " 197: 0.003069985869766389,\n",
       " 198: 0.003316189577819341,\n",
       " 199: 0.003384836744236882,\n",
       " 200: 0.003094813253316775,\n",
       " 201: 0.003350367512033832,\n",
       " 202: 0.0033831125752000037,\n",
       " 203: 0.0026619858275222412,\n",
       " 204: 0.0024306392916989368,\n",
       " 205: 0.0031741211052059843,\n",
       " 206: 0.003373610789581491,\n",
       " 207: 0.002843535216500433,\n",
       " 208: 0.0027816910267034734,\n",
       " 209: 0.0032259988627122036,\n",
       " 210: 0.0029206751071010977,\n",
       " 211: 0.0032926798973500664,\n",
       " 212: 0.0031470695834793765,\n",
       " 213: 0.002772678721984702,\n",
       " 214: 0.0030894856805812982,\n",
       " 215: 0.0028116384659956544,\n",
       " 216: 0.0027353729085953806,\n",
       " 217: 0.0031035682210560104,\n",
       " 218: 0.003004399446466842,\n",
       " 219: 0.0031215450207465966,\n",
       " 220: 0.0033996773584444714,\n",
       " 221: 0.003218787243757743,\n",
       " 222: 0.0031774439324715924,\n",
       " 223: 0.0031696458509072067,\n",
       " 224: 0.0027600929665731015,\n",
       " 225: 0.0025046639092928666,\n",
       " 226: 0.0023343929090167332,\n",
       " 227: 0.002647604406168134,\n",
       " 228: 0.0029579074884521373,\n",
       " 229: 0.0029295861189343885,\n",
       " 230: 0.003018248964658098,\n",
       " 231: 0.0029875557592531285,\n",
       " 232: 0.002844245633475748,\n",
       " 233: 0.003347326087436476,\n",
       " 234: 0.003496323697622874,\n",
       " 235: 0.0029528769366813827,\n",
       " 236: 0.0030277516621080164,\n",
       " 237: 0.003198722834924469,\n",
       " 238: 0.0033931696981667975,\n",
       " 239: 0.0031747267791469836,\n",
       " 240: 0.002764503635450738,\n",
       " 241: 0.00254197092926093,\n",
       " 242: 0.002394524385281561,\n",
       " 243: 0.0032881958681956975,\n",
       " 244: 0.002884882478971825,\n",
       " 245: 0.002240218243405569,\n",
       " 246: 0.0022214067198593245,\n",
       " 247: 0.0019984655076597563,\n",
       " 248: 0.0025650889824704354,\n",
       " 249: 0.0027139653085686702,\n",
       " 250: 0.002757050712300186,\n",
       " 251: 0.002829217130752064,\n",
       " 252: 0.0028525500840364646,\n",
       " 253: 0.002809642372607116,\n",
       " 254: 0.002979505535037736,\n",
       " 255: 0.003269263856611187,\n",
       " 256: 0.0027051719043748703,\n",
       " 257: 0.0022860989346923074,\n",
       " 258: 0.0020553798308545674,\n",
       " 259: 0.002473853538726453,\n",
       " 260: 0.00252069937597937,\n",
       " 261: 0.002894032763349416,\n",
       " 262: 0.003403831042196992,\n",
       " 263: 0.003334942650885108,\n",
       " 264: 0.0031343168033041166,\n",
       " 265: 0.0031542396012790713,\n",
       " 266: 0.0031626392487828668,\n",
       " 267: 0.0032694257179404166,\n",
       " 268: 0.0030575033752534273,\n",
       " 269: 0.0035704468878875605,\n",
       " 270: 0.003555000056096965,\n",
       " 271: 0.0029569575342801415,\n",
       " 272: 0.002450971080691085,\n",
       " 273: 0.002235129871796975,\n",
       " 274: 0.0023096311259977647,\n",
       " 275: 0.0028681497345973746,\n",
       " 276: 0.0025184001212953055,\n",
       " 277: 0.0019808472253634553,\n",
       " 278: 0.001975139619445096,\n",
       " 279: 0.0017288778274189218,\n",
       " 280: 0.0022559313481994485,\n",
       " 281: 0.0020943036913381253,\n",
       " 282: 0.0023865073402186965,\n",
       " 283: 0.002633845525514028,\n",
       " 284: 0.0027252450760885073,\n",
       " 285: 0.0026977820065622548,\n",
       " 286: 0.002910013080728557,\n",
       " 287: 0.00331559454023617,\n",
       " 288: 0.002732312971019878,\n",
       " 289: 0.0024187134728677596,\n",
       " 290: 0.0023628075895140363,\n",
       " 291: 0.0026954275227406193,\n",
       " 292: 0.0027838254000516357,\n",
       " 293: 0.0031993604222350014,\n",
       " 294: 0.002977655343949789,\n",
       " 295: 0.0031169165933534566,\n",
       " 296: 0.0026889608107643613,\n",
       " 297: 0.0027329107878223766,\n",
       " 298: 0.0027380882229615317,\n",
       " 299: 0.003144058911241759,\n",
       " 300: 0.0031013242846257734,\n",
       " 301: 0.0038722112812914437,\n",
       " 302: 0.0038950606433916036,\n",
       " 303: 0.0032557751969820225,\n",
       " 304: 0.002541139578920764,\n",
       " 305: 0.002475473085543531,\n",
       " 306: 0.0023532938102731743,\n",
       " 307: 0.0028563601335805685,\n",
       " 308: 0.003149051964007629,\n",
       " 309: 0.0025287148411140387,\n",
       " 310: 0.002362692149906254,\n",
       " 311: 0.0015174573198583612,\n",
       " 312: 0.0018802054733842617,\n",
       " 313: 0.0015694781369480588,\n",
       " 314: 0.0017840728469199624,\n",
       " 315: 0.002081022778855915,\n",
       " 316: 0.002838714717224457,\n",
       " 317: 0.0030007841256032178,\n",
       " 318: 0.0031247254971507902,\n",
       " 319: 0.0035259112292615577,\n",
       " 320: 0.002848281510022355,\n",
       " 321: 0.002594477351640133,\n",
       " 322: 0.0029434881195554044,\n",
       " 323: 0.0032364567110623565,\n",
       " 324: 0.0030831933494105575,\n",
       " 325: 0.0032310857634565582,\n",
       " 326: 0.002750288473622096,\n",
       " 327: 0.0026838048640755534,\n",
       " 328: 0.002722003600460459,\n",
       " 329: 0.0023685533300164927,\n",
       " 330: 0.0021825654481437804,\n",
       " 331: 0.0027802397008679456,\n",
       " 332: 0.0034457178581373316,\n",
       " 333: 0.004023194782577552,\n",
       " 334: 0.0038708989720418717,\n",
       " 335: 0.00323823145725482,\n",
       " 336: 0.002287317498293692,\n",
       " 337: 0.002318130048534101,\n",
       " 338: 0.002433852370406317,\n",
       " 339: 0.0026934813504711132,\n",
       " 340: 0.0027584903552776052,\n",
       " 341: 0.0027297048515511966,\n",
       " 342: 0.002754174424279449,\n",
       " 343: 0.0018284544943055453,\n",
       " 344: 0.0025687462410479813,\n",
       " 345: 0.002137746242185295,\n",
       " 346: 0.0016585331054797682,\n",
       " 347: 0.0019635606562283265,\n",
       " 348: 0.002965447396515697,\n",
       " 349: 0.0034959056529631645,\n",
       " 350: 0.0032976726367563596,\n",
       " 351: 0.003397581341112968,\n",
       " 352: 0.002980438421150079,\n",
       " 353: 0.002713949212692172,\n",
       " 354: 0.002996241630952155,\n",
       " 355: 0.003701853980418161,\n",
       " 356: 0.00358635392375621,\n",
       " 357: 0.0034039820335491243,\n",
       " 358: 0.0025199021832253646,\n",
       " 359: 0.0020855603536544727,\n",
       " 360: 0.0023895428446359477,\n",
       " 361: 0.0021798760221613204,\n",
       " 362: 0.0024375002850593767,\n",
       " 363: 0.003109921439581629,\n",
       " 364: 0.00374272665733255,\n",
       " 365: 0.0035780379902969855,\n",
       " 366: 0.003952632238182379,\n",
       " 367: 0.0028942330220534007,\n",
       " 368: 0.0021189786935863976,\n",
       " 369: 0.002378903514426939,\n",
       " 370: 0.0028860078349370526,\n",
       " 371: 0.0030493517506291626,\n",
       " 372: 0.0025853815440794827,\n",
       " 373: 0.0026968609123709567,\n",
       " 374: 0.002909539258404593,\n",
       " 375: 0.002422530353055999,\n",
       " 376: 0.0024824583960724638,\n",
       " 377: 0.002278408602550779,\n",
       " 378: 0.0017212956997563888,\n",
       " 379: 0.002181713452731156,\n",
       " 380: 0.002819355220027501,\n",
       " 381: 0.003198285658263315,\n",
       " 382: 0.0033528913444358676,\n",
       " 383: 0.0030824931524963822,\n",
       " 384: 0.00306713867820566,\n",
       " 385: 0.003066161964378225,\n",
       " 386: 0.0031937567846669467,\n",
       " 387: 0.0036001481960041487,\n",
       " 388: 0.0035754584015069146,\n",
       " 389: 0.0034107499515845232,\n",
       " 390: 0.0030857444138106278,\n",
       " 391: 0.0023377838821311593,\n",
       " 392: 0.001995717400496061,\n",
       " 393: 0.002270426685959299,\n",
       " 394: 0.0033009733520239404,\n",
       " 395: 0.003995516801571046,\n",
       " 396: 0.003629825636566447,\n",
       " 397: 0.003754760849058906,\n",
       " 398: 0.004273055921236147,\n",
       " 399: 0.0035007252015500766,\n",
       " 400: 0.0031559476669977002,\n",
       " 401: 0.0033608809913459464,\n",
       " 402: 0.0033915365121391086,\n",
       " 403: 0.0035447561859303267,\n",
       " 404: 0.0026854319255918157,\n",
       " 405: 0.0028591733796436442,\n",
       " 406: 0.0031358797919100703,\n",
       " 407: 0.0033689821972107572,\n",
       " 408: 0.0030614472754215446,\n",
       " 409: 0.002582497732186525,\n",
       " 410: 0.00235835025295858,\n",
       " 411: 0.002632237436744922,\n",
       " 412: 0.003229490463665503,\n",
       " 413: 0.0037145040121230564,\n",
       " 414: 0.0036304270720164313,\n",
       " 415: 0.0035320084516793217,\n",
       " 416: 0.003388295450081837,\n",
       " 417: 0.0033591059681875256,\n",
       " 418: 0.0036309556894053158,\n",
       " 419: 0.0038180717968320944,\n",
       " 420: 0.0041556942891733195,\n",
       " 421: 0.003957847074390641,\n",
       " 422: 0.0031887520363192644,\n",
       " 423: 0.0029107566079129917,\n",
       " 424: 0.002876075258471634,\n",
       " 425: 0.00276571862792125,\n",
       " 426: 0.0033912510110073947,\n",
       " 427: 0.003958444002540427,\n",
       " 428: 0.0034414983173895837,\n",
       " 429: 0.0037485477785648156,\n",
       " 430: 0.0038724388824450625,\n",
       " 431: 0.003960023133607358,\n",
       " 432: 0.003914488851561398,\n",
       " 433: 0.0035450611718389606,\n",
       " 434: 0.0036186160492751073,\n",
       " 435: 0.0038094208162110415,\n",
       " 436: 0.003464789561641398,\n",
       " 437: 0.0031247644177867305,\n",
       " 438: 0.003248611469967147,\n",
       " 439: 0.0035284044294171947,\n",
       " 440: 0.0034326350008200765,\n",
       " 441: 0.0030926640345924515,\n",
       " 442: 0.0028304701732530194,\n",
       " 443: 0.0027023242219262637,\n",
       " 444: 0.0035267071909875097,\n",
       " 445: 0.004311208901216738,\n",
       " 446: 0.003758745723952175,\n",
       " 447: 0.0036569665377434194,\n",
       " 448: 0.0036663167008121975,\n",
       " 449: 0.0036735172262903153,\n",
       " 450: 0.0036730173730660505,\n",
       " 451: 0.003675663720330165,\n",
       " 452: 0.00403834644332043,\n",
       " 453: 0.0037526485225164103,\n",
       " 454: 0.003061203031210257,\n",
       " 455: 0.0028200788728840157,\n",
       " 456: 0.002805157529357318,\n",
       " 457: 0.0026833271947106756,\n",
       " 458: 0.0033219473923725997,\n",
       " 459: 0.003599700209767879,\n",
       " 460: 0.003628639633025852,\n",
       " 461: 0.003634740958841927,\n",
       " 462: 0.0038541166816345564,\n",
       " 463: 0.003547108716482633,\n",
       " 464: 0.003388281679079808,\n",
       " 465: 0.0031918942877940614,\n",
       " 466: 0.003677104992010201,\n",
       " 467: 0.003726804108143556,\n",
       " 468: 0.0033431438986780485,\n",
       " 469: 0.0032155583021931276,\n",
       " 470: 0.0036837263405702218,\n",
       " 471: 0.0037825578624758743,\n",
       " 472: 0.003079656007695959,\n",
       " 473: 0.0028012177166658607,\n",
       " 474: 0.0022903204497521715,\n",
       " 475: 0.0021065691680418053,\n",
       " 476: 0.003305530457360208,\n",
       " 477: 0.004370956786028894,\n",
       " 478: 0.0034108430828102086,\n",
       " 479: 0.003175209240999483,\n",
       " 480: 0.0037254307530226326,\n",
       " 481: 0.0034587779299112514,\n",
       " 482: 0.0032494614517708147,\n",
       " 483: 0.003546101821536902,\n",
       " 484: 0.0037729296863289394,\n",
       " 485: 0.003215873759531304,\n",
       " 486: 0.002409210613432517,\n",
       " 487: 0.002534796116871145,\n",
       " 488: 0.0028402331226348334,\n",
       " 489: 0.0026390277056021387,\n",
       " 490: 0.0029606405559324134,\n",
       " 491: 0.0032272758300766016,\n",
       " 492: 0.0031052669850547146,\n",
       " 493: 0.003148388626157548,\n",
       " 494: 0.003418205609135762,\n",
       " 495: 0.003136349772058299,\n",
       " 496: 0.0029478531697421373,\n",
       " 497: 0.003288060682788009,\n",
       " 498: 0.004051585242778292,\n",
       " 499: 0.0040938485636652965,\n",
       " 500: 0.004080237017708701,\n",
       " 501: 0.0040243016678034454,\n",
       " 502: 0.00413077118020962,\n",
       " 503: 0.00412070685761462,\n",
       " 504: 0.003117948559721342,\n",
       " 505: 0.0029952875494014468,\n",
       " 506: 0.003059266627420399,\n",
       " 507: 0.0024041449871734528,\n",
       " 508: 0.0027793750784587687,\n",
       " 509: 0.0037394423074433025,\n",
       " 510: 0.0027651700988069574,\n",
       " 511: 0.0027650434934252187,\n",
       " 512: 0.0035006583306014653,\n",
       " 513: 0.003564218788086485,\n",
       " 514: 0.0033045203538692672,\n",
       " 515: 0.0034707081883802293,\n",
       " 516: 0.0037574384639132534,\n",
       " 517: 0.003250990339511625,\n",
       " 518: 0.0023856689871372097,\n",
       " 519: 0.002380470928247224,\n",
       " 520: 0.0029428789394284746,\n",
       " 521: 0.0026799259980996963,\n",
       " 522: 0.0024879466698846313,\n",
       " 523: 0.002649078753668466,\n",
       " 524: 0.0025486732844730116,\n",
       " 525: 0.0034003776607340647,\n",
       " 526: 0.003827887669708257,\n",
       " 527: 0.0034804152625982306,\n",
       " 528: 0.003157665999360619,\n",
       " 529: 0.0034682513406753585,\n",
       " 530: 0.003971967087289424,\n",
       " 531: 0.004097297093613972,\n",
       " 532: 0.004014896642499961,\n",
       " 533: 0.0037872116916803566,\n",
       " 534: 0.0037426341033249145,\n",
       " 535: 0.00343590867498211,\n",
       " 536: 0.002547616241657611,\n",
       " 537: 0.0031329756559517054,\n",
       " 538: 0.003311904385371468,\n",
       " 539: 0.0027053641628954125,\n",
       " 540: 0.002666030849651527,\n",
       " 541: 0.0030351179355959075,\n",
       " 542: 0.002548618662677788,\n",
       " 543: 0.002699098677880177,\n",
       " 544: 0.0034707050020320812,\n",
       " 545: 0.003703254538143629,\n",
       " 546: 0.003355078977978368,\n",
       " 547: 0.003231657403707977,\n",
       " 548: 0.003490647237436779,\n",
       " 549: 0.003462120110647246,\n",
       " 550: 0.0026905672531036266,\n",
       " 551: 0.0022318365911439225,\n",
       " 552: 0.002307648684397662,\n",
       " 553: 0.002312818362790856,\n",
       " 554: 0.0022781238161965166,\n",
       " 555: 0.0026117426781348064,\n",
       " 556: 0.0027131624395875754,\n",
       " 557: 0.003126871812960712,\n",
       " 558: 0.0036684848164081317,\n",
       " 559: 0.002978574580460908,\n",
       " 560: 0.00277236400515746,\n",
       " 561: 0.002952651307725706,\n",
       " 562: 0.0033358197269690933,\n",
       " 563: 0.0033407458237076303,\n",
       " 564: 0.0032259392266261026,\n",
       " 565: 0.0034732127211278153,\n",
       " 566: 0.003590101308541058,\n",
       " 567: 0.003012320983617269,\n",
       " 568: 0.0022020790432657534,\n",
       " 569: 0.0024946239461582195,\n",
       " 570: 0.002760347663095183,\n",
       " 571: 0.0027067910584513376,\n",
       " 572: 0.002831281106541958,\n",
       " 573: 0.0028559176216095042,\n",
       " 574: 0.002456091958478356,\n",
       " 575: 0.0027077070876311677,\n",
       " 576: 0.0032605527782138366,\n",
       " 577: 0.003033642021958467,\n",
       " 578: 0.002619133564875673,\n",
       " 579: 0.0028574616772156316,\n",
       " 580: 0.0034787782209224536,\n",
       " 581: 0.0038832575725114725,\n",
       " 582: 0.003418751723557595,\n",
       " 583: 0.0030875656715511524,\n",
       " 584: 0.003114085030072486,\n",
       " 585: 0.0029637116282793708,\n",
       " 586: 0.0028509878852836096,\n",
       " 587: 0.002610629608530415,\n",
       " 588: 0.003047806037670282,\n",
       " 589: 0.0030665956012238395,\n",
       " 590: 0.003062117311624455,\n",
       " 591: 0.002477784132259998,\n",
       " 592: 0.002020821120637961,\n",
       " 593: 0.0019661464674247757,\n",
       " 594: 0.0021371555185723716,\n",
       " 595: 0.0020193024938024045,\n",
       " 596: 0.0019633532032061873,\n",
       " 597: 0.0028187763718765025,\n",
       " 598: 0.003352800745313157,\n",
       " 599: 0.0028377219928518243,\n",
       " 600: 0.0021727912195812957,\n",
       " 601: 0.002206587303869659,\n",
       " 602: 0.002640109756015234,\n",
       " 603: 0.0027564477508564686,\n",
       " 604: 0.0026912711770270656,\n",
       " 605: 0.0029498012627076162,\n",
       " 606: 0.002639750238129979,\n",
       " 607: 0.0026428862665789153,\n",
       " 608: 0.0027560671996612355,\n",
       " 609: 0.0025135184828503996,\n",
       " 610: 0.0022888201559134007,\n",
       " 611: 0.0024153504534552206,\n",
       " 612: 0.002942874167681429,\n",
       " 613: 0.003112110153138626,\n",
       " 614: 0.0029717993593257968,\n",
       " 615: 0.0031536354780267836,\n",
       " 616: 0.003519793948399255,\n",
       " 617: 0.0031646820827610847,\n",
       " 618: 0.003037314966017437,\n",
       " 619: 0.00329284438788954,\n",
       " 620: 0.0029940853056151843,\n",
       " 621: 0.00300689628395372,\n",
       " 622: 0.00314508116767667,\n",
       " 623: 0.0031028963332710813,\n",
       " 624: 0.0028229481301327744,\n",
       " 625: 0.0025589417733346137,\n",
       " 626: 0.0027852032947028385,\n",
       " 627: 0.0025693170451002254,\n",
       " 628: 0.001971879395317651,\n",
       " 629: 0.002730476590590828,\n",
       " 630: 0.002735527999700099,\n",
       " 631: 0.002471212432399421,\n",
       " 632: 0.0022484591399749023,\n",
       " 633: 0.0025492732877838604,\n",
       " 634: 0.0031802918549401456,\n",
       " 635: 0.0031193009898747657,\n",
       " 636: 0.0027037722679497775,\n",
       " 637: 0.002846399456791286,\n",
       " 638: 0.002697811833698655,\n",
       " 639: 0.0026500544231048756,\n",
       " 640: 0.002405008634718721,\n",
       " 641: 0.0023431155194288834,\n",
       " 642: 0.0023238190283713235,\n",
       " 643: 0.002148004377224766,\n",
       " 644: 0.0027382726814924995,\n",
       " 645: 0.0024839633556080617,\n",
       " 646: 0.0022709148359339477,\n",
       " 647: 0.0023409494006690615,\n",
       " 648: 0.0027488274230372508,\n",
       " 649: 0.0031197968482138618,\n",
       " 650: 0.0029603904688275096,\n",
       " 651: 0.0031366454602066228,\n",
       " 652: 0.0025170915870117755,\n",
       " 653: 0.002436268224763164,\n",
       " 654: 0.0026246065617786295,\n",
       " 655: 0.002778537817607151,\n",
       " 656: 0.0025931756714760706,\n",
       " 657: 0.0027068057363824876,\n",
       " 658: 0.0033875575612084406,\n",
       " 659: 0.0036137957293831914,\n",
       " 660: 0.002975917028687812,\n",
       " 661: 0.0025987727905588177,\n",
       " 662: 0.0024824443277725447,\n",
       " 663: 0.0026738765154199736,\n",
       " 664: 0.002549862124938492,\n",
       " 665: 0.003222281247749578,\n",
       " 666: 0.0037178800305226185,\n",
       " 667: 0.0033324319702709537,\n",
       " 668: 0.002924600079797464,\n",
       " 669: 0.0026073767393657733,\n",
       " 670: 0.0019261881336925441,\n",
       " 671: 0.002727980809316844,\n",
       " 672: 0.002453890591674084,\n",
       " 673: 0.0020952955813216215,\n",
       " 674: 0.002228887051629365,\n",
       " 675: 0.0021135858430808645,\n",
       " 676: 0.00248347098805096,\n",
       " 677: 0.00249387169260136,\n",
       " 678: 0.00256490059250813,\n",
       " 679: 0.0021107253806729835,\n",
       " 680: 0.0021468482931125887,\n",
       " 681: 0.0025607764714854686,\n",
       " 682: 0.002156027811629726,\n",
       " 683: 0.0021098525291512026,\n",
       " 684: 0.0019410766546624628,\n",
       " 685: 0.0020647183227088237,\n",
       " 686: 0.002385682819355406,\n",
       " 687: 0.0027289891046176798,\n",
       " 688: 0.0023174239991539543,\n",
       " 689: 0.003029909160098738,\n",
       " 690: 0.0032878697751142854,\n",
       " 691: 0.0036107431644774287,\n",
       " 692: 0.0034397428436159475,\n",
       " 693: 0.003259792378083657,\n",
       " 694: 0.002870766094750429,\n",
       " 695: 0.0025846591165702318,\n",
       " 696: 0.002954110150866282,\n",
       " 697: 0.0036592337540070287,\n",
       " 698: 0.003688995113180564,\n",
       " 699: 0.003167312923599455,\n",
       " 700: 0.002648664086714178,\n",
       " 701: 0.002225481545693978,\n",
       " 702: 0.0015414757345828566,\n",
       " 703: 0.0024210358092326524,\n",
       " 704: 0.002516934791987638,\n",
       " 705: 0.00214842109412758,\n",
       " 706: 0.0021208412783215386,\n",
       " 707: 0.0015994086421093128,\n",
       " 708: 0.002048935458740114,\n",
       " 709: 0.002627291012921346,\n",
       " 710: 0.0024851319449336463,\n",
       " 711: 0.001905879116785447,\n",
       " 712: 0.00225165446439409,\n",
       " 713: 0.0028281410461262826,\n",
       " 714: 0.0019942589060426372,\n",
       " 715: 0.0021366542959923183,\n",
       " 716: 0.0024603128228206732,\n",
       " 717: 0.002261404926225966,\n",
       " 718: 0.0022436279695335214,\n",
       " 719: 0.002075758370225068,\n",
       " 720: 0.0022999974101667975,\n",
       " 721: 0.003257491905371899,\n",
       " 722: 0.003259292924478394,\n",
       " 723: 0.003272119656682002,\n",
       " 724: 0.0032941928035605914,\n",
       " 725: 0.0033031210364335836,\n",
       " 726: 0.0027718314971217077,\n",
       " 727: 0.002526582306842835,\n",
       " 728: 0.002740033024607874,\n",
       " 729: 0.003326827467237376,\n",
       " 730: 0.002800367383727703,\n",
       " 731: 0.0025017112544422223,\n",
       " 732: 0.0024530295065777062,\n",
       " 733: 0.002146114038688982,\n",
       " 734: 0.0019191360186351263,\n",
       " 735: 0.002701029310659807,\n",
       " 736: 0.002794516889491782,\n",
       " 737: 0.0026750533265036945,\n",
       " 738: 0.0029499131358486676,\n",
       " 739: 0.002573806211789745,\n",
       " 740: 0.0026512068647311943,\n",
       " 741: 0.0030505513987203994,\n",
       " 742: 0.003254400466040937,\n",
       " 743: 0.003218664893216487,\n",
       " 744: 0.0033104494512390358,\n",
       " 745: 0.00335270259532402,\n",
       " 746: 0.002549882191507846,\n",
       " 747: 0.0026031060539066244,\n",
       " 748: 0.0027258831400973254,\n",
       " 749: 0.002550513746838465,\n",
       " 750: 0.002364604480392129,\n",
       " 751: 0.0022230191037986547,\n",
       " 752: 0.0025518270121792645,\n",
       " 753: 0.0025893817710969835,\n",
       " 754: 0.002465010412986663,\n",
       " 755: 0.0030191036216555243,\n",
       " 756: 0.0032887341223131724,\n",
       " 757: 0.0029335756267168736,\n",
       " 758: 0.002464061126767303,\n",
       " 759: 0.0028704600008069426,\n",
       " 760: 0.0029183350869549044,\n",
       " 761: 0.003032405407847338,\n",
       " 762: 0.002610216102037903,\n",
       " 763: 0.002570989062369727,\n",
       " 764: 0.0024837589088983463,\n",
       " 765: 0.002277476205909959,\n",
       " 766: 0.0023626045332106496,\n",
       " 767: 0.0026700437458850317,\n",
       " 768: 0.0029819485029526846,\n",
       " 769: 0.0028614833020015416,\n",
       " 770: 0.0036304873165082,\n",
       " 771: 0.003427867140256045,\n",
       " 772: 0.003372486103765391,\n",
       " 773: 0.003567177423846406,\n",
       " 774: 0.0038937093399853914,\n",
       " 775: 0.003986925962295643,\n",
       " 776: 0.0036653393845634354,\n",
       " 777: 0.003335953838671354,\n",
       " 778: 0.0029894721156247315,\n",
       " 779: 0.0034182937051449483,\n",
       " 780: 0.003356639429571498,\n",
       " 781: 0.0031222693866211643,\n",
       " 782: 0.002908807821859508,\n",
       " 783: 0.003084379563155099,\n",
       " 784: 0.0034350904709282357,\n",
       " 785: 0.0031167167712018405,\n",
       " 786: 0.003461995402321006,\n",
       " 787: 0.004023260986991185,\n",
       " 788: 0.003524673829223544,\n",
       " 789: 0.0030498984069722342,\n",
       " 790: 0.002474866557993585,\n",
       " 791: 0.002817641087720067,\n",
       " 792: 0.0028088207514471997,\n",
       " 793: 0.002308491901653694,\n",
       " 794: 0.002513615264918365,\n",
       " 795: 0.002874024952981964,\n",
       " 796: 0.00291046479936862,\n",
       " 797: 0.00306924960661783,\n",
       " 798: 0.0030798921180828953,\n",
       " 799: 0.002943656289387184,\n",
       " 800: 0.0028362251812348396,\n",
       " 801: 0.0027945909083666514,\n",
       " 802: 0.003312827485926674,\n",
       " 803: 0.0029675114803670117,\n",
       " 804: 0.0031483628468727417,\n",
       " 805: 0.0033150705330022416,\n",
       " 806: 0.00355207502144534,\n",
       " 807: 0.0038308403860945915,\n",
       " 808: 0.0035165364666255076,\n",
       " 809: 0.003035203694172299,\n",
       " 810: 0.0028814602578607702,\n",
       " 811: 0.003853794024758887,\n",
       " 812: 0.004112487720563691,\n",
       " 813: 0.00336882101573288,\n",
       " 814: 0.0030649520694955284,\n",
       " 815: 0.003481189861438995,\n",
       " 816: 0.0036776720958399257,\n",
       " 817: 0.003566125935451675,\n",
       " 818: 0.00448101275627979,\n",
       " 819: 0.004516228609644523,\n",
       " 820: 0.0035097264714446315,\n",
       " 821: 0.003144253590880501,\n",
       " 822: 0.002809478954871882,\n",
       " 823: 0.003172859060096534,\n",
       " 824: 0.002910107246241512,\n",
       " 825: 0.002185948896522723,\n",
       " 826: 0.0024498209511662123,\n",
       " 827: 0.0031427930491381337,\n",
       " 828: 0.0032639089942414494,\n",
       " 829: 0.003376414385563168,\n",
       " 830: 0.0031796973029709266,\n",
       " 831: 0.003018069708878849,\n",
       " 832: 0.002930327797474194,\n",
       " 833: 0.0025575383467269733,\n",
       " 834: 0.002505351596520908,\n",
       " 835: 0.0024083566958550922,\n",
       " 836: 0.0028107001284659954,\n",
       " 837: 0.0030180069100174227,\n",
       " 838: 0.0029009953759104035,\n",
       " 839: 0.002834558541394996,\n",
       " 840: 0.002434474582767092,\n",
       " 841: 0.0021322274939382926,\n",
       " 842: 0.0025281331169029237,\n",
       " 843: 0.0030546840826854785,\n",
       " 844: 0.00362304589852563,\n",
       " 845: 0.0036709196731028776,\n",
       " 846: 0.0034567482129442997,\n",
       " 847: 0.003444560483794762,\n",
       " 848: 0.003440410872987675,\n",
       " 849: 0.003173431605205272,\n",
       " 850: 0.003433232798610052,\n",
       " 851: 0.0033236528838239735,\n",
       " 852: 0.0026144953986781527,\n",
       " 853: 0.002878305489344781,\n",
       " 854: 0.002805189141314613,\n",
       " 855: 0.0030304519181207344,\n",
       " 856: 0.002672321452699806,\n",
       " 857: 0.002133427231439756,\n",
       " 858: 0.0023272392832608597,\n",
       " 859: 0.0029428384109898417,\n",
       " 860: 0.0030476670008397952,\n",
       " 861: 0.0032622987155328954,\n",
       " 862: 0.003000698127462819,\n",
       " 863: 0.0028028994021855776,\n",
       " 864: 0.002931919464369005,\n",
       " 865: 0.002501536994884104,\n",
       " 866: 0.0023584641338924473,\n",
       " 867: 0.0019337113394968498,\n",
       " 868: 0.0022333691040658608,\n",
       " 869: 0.002664009924667704,\n",
       " 870: 0.0023906112089015492,\n",
       " 871: 0.0020059907939256527,\n",
       " 872: 0.0021879320522865202,\n",
       " 873: 0.002098068856825374,\n",
       " 874: 0.002290308142502781,\n",
       " 875: 0.0023081580932246613,\n",
       " 876: 0.002568270853804856,\n",
       " 877: 0.0030494822994217518,\n",
       " 878: 0.003040384754615589,\n",
       " 879: 0.0028214177280912888,\n",
       " 880: 0.002544390773982362,\n",
       " 881: 0.002323704200699398,\n",
       " 882: 0.0025661853657487008,\n",
       " 883: 0.002441434877927779,\n",
       " 884: 0.0022589732582733147,\n",
       " 885: 0.002502338684870624,\n",
       " 886: 0.002819989768028029,\n",
       " 887: 0.0028115091496581156,\n",
       " 888: 0.0025809057343648503,\n",
       " 889: 0.00222244402640218,\n",
       " 890: 0.0027486732746754773,\n",
       " 891: 0.00297171604537527,\n",
       " 892: 0.002745033684117574,\n",
       " 893: 0.0028859445990414437,\n",
       " 894: 0.0030461441389318784,\n",
       " 895: 0.0032053388375376302,\n",
       " 896: 0.002894952890599648,\n",
       " 897: 0.002371394772108018,\n",
       " 898: 0.0023000242988871016,\n",
       " 899: 0.0019143439573115035,\n",
       " 900: 0.0016667938901390482,\n",
       " 901: 0.0019758401906061576,\n",
       " 902: 0.0019460605518682554,\n",
       " 903: 0.0018029700996560747,\n",
       " 904: 0.0023205481547665173,\n",
       " 905: 0.002569650465085061,\n",
       " 906: 0.0027364246295944767,\n",
       " 907: 0.0024407058015507486,\n",
       " 908: 0.002367490863100338,\n",
       " 909: 0.002552051837395223,\n",
       " 910: 0.0026517264493815815,\n",
       " 911: 0.002095067016732362,\n",
       " 912: 0.0016016443454711848,\n",
       " 913: 0.0019269348697750282,\n",
       " 914: 0.002271025434415309,\n",
       " 915: 0.002149848136646003,\n",
       " 916: 0.002415117670104009,\n",
       " 917: 0.0028215214959026137,\n",
       " 918: 0.002885113249373926,\n",
       " 919: 0.0027905894663986463,\n",
       " 920: 0.002742016446945817,\n",
       " 921: 0.002392487606220161,\n",
       " 922: 0.002394362256129968,\n",
       " 923: 0.0027849958845937474,\n",
       " 924: 0.0027524665774606165,\n",
       " 925: 0.002828297094356772,\n",
       " 926: 0.0027700191534305914,\n",
       " 927: 0.0030041983131245914,\n",
       " 928: 0.0026327961663881235,\n",
       " 929: 0.0021622506428886318,\n",
       " 930: 0.0023423081133876304,\n",
       " 931: 0.0023623724428266917,\n",
       " 932: 0.0022826172835694034,\n",
       " 933: 0.002443941497604205,\n",
       " 934: 0.00240209680060409,\n",
       " 935: 0.002246009220979422,\n",
       " 936: 0.002528509608007735,\n",
       " 937: 0.0028142925314080854,\n",
       " 938: 0.0026485285384437542,\n",
       " 939: 0.00224703890423436,\n",
       " 940: 0.002176975193123487,\n",
       " 941: 0.0026173308792545942,\n",
       " 942: 0.002455286015717673,\n",
       " 943: 0.002345628034982626,\n",
       " 944: 0.002029030891221678,\n",
       " 945: 0.0019103788941480541,\n",
       " 946: 0.0020886856675090405,\n",
       " 947: 0.002210210743999657,\n",
       " 948: 0.0024534493236879197,\n",
       " 949: 0.0030066439254015954,\n",
       " 950: 0.0027037760737657463,\n",
       " 951: 0.0025684199342978346,\n",
       " 952: 0.002681275872098819,\n",
       " 953: 0.0025702157755374136,\n",
       " 954: 0.0026012833570524378,\n",
       " 955: 0.0030485866063707204,\n",
       " 956: 0.0028704908367986597,\n",
       " 957: 0.002736432617333611,\n",
       " 958: 0.002679408870094296,\n",
       " 959: 0.0024126334888368533,\n",
       " 960: 0.002283019653057944,\n",
       " 961: 0.0019133470811661208,\n",
       " 962: 0.002299362647621379,\n",
       " 963: 0.0025179160513801855,\n",
       " 964: 0.0022058288624704904,\n",
       " 965: 0.002645004758542213,\n",
       " 966: 0.0027992067339536967,\n",
       " 967: 0.0026984000421361653,\n",
       " 968: 0.002645516849282271,\n",
       " 969: 0.0028914123073262806,\n",
       " 970: 0.0025650671289567275,\n",
       " 971: 0.0023505651494773695,\n",
       " 972: 0.0027259411153798675,\n",
       " 973: 0.0027540649494061553,\n",
       " 974: 0.002341874478306455,\n",
       " 975: 0.0023644118605721767,\n",
       " 976: 0.0024844056820792137,\n",
       " 977: 0.0023641248101330023,\n",
       " 978: 0.002625487875284088,\n",
       " 979: 0.002949812536483604,\n",
       " 980: 0.0028330804506913627,\n",
       " 981: 0.00281451991194934,\n",
       " 982: 0.002463046625356715,\n",
       " 983: 0.0025091757087195195,\n",
       " 984: 0.0025119489648085225,\n",
       " 985: 0.0023629499879317486,\n",
       " 986: 0.002721335739421269,\n",
       " 987: 0.003009079853086922,\n",
       " 988: 0.002605470178177778,\n",
       " 989: 0.0025725218931243604,\n",
       " 990: 0.0027603508773772507,\n",
       " 991: 0.00252079180240529,\n",
       " 992: 0.0021685367147740507,\n",
       " 993: 0.002055695586930476,\n",
       " 994: 0.002324428030852046,\n",
       " 995: 0.0024565772129363688,\n",
       " 996: 0.0021815320025560738,\n",
       " 997: 0.0024485678913721436,\n",
       " 998: 0.00271727888967076,\n",
       " 999: 0.0023013552748749775,\n",
       " ...}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "23a87aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.91567867836909\n"
     ]
    }
   ],
   "source": [
    "norms = [np.linalg.norm(x) for x in train_x]\n",
    "print(max(norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "103fd4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.000000000000067\n",
      "curr trial is 0\n"
     ]
    }
   ],
   "source": [
    "# PAC MEAN\n",
    "\n",
    "train_x, train_y, test_x, test_y, num_classes, train_len = gen_cifar10(normalize=True)\n",
    "true_mean = np.average(train_x, axis=0)\n",
    "subsample_rate = int(0.5*train_len)\n",
    "\n",
    "clipped_train_x = np.array([clip_to_threshold(train_x[i], 3) for i in range(len(train_x))])\n",
    "norms = [np.linalg.norm(x) for x in clipped_train_x]\n",
    "print(max(norms))\n",
    "clipped_noise = hybrid_noise_auto(clipped_train_x, train_y, subsample_rate, num_classes, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8117d6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0801084004786155"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clipped_noise.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccf42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_dists = {}\n",
    "num_trials = 1000\n",
    "\n",
    "for mi in mi_range:\n",
    "    scaled_noise = {k: noise[k] * (0.5 / mi) for k in noise}\n",
    "    iso_noise = max(scaled_noise.values())\n",
    "    iso_scaled = {k: iso_noise for k in noise}\n",
    "    avg_dist_pac = 0\n",
    "    avg_iso_dist_pac = 0\n",
    "    subsampled_dist = 0\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        subsampled_dist += np.linalg.norm(released_mean - true_mean)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=scaled_noise[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    for _ in range(num_trials):\n",
    "        shuffled_x1, shuffled_y1 = shuffle(train_x, train_y)\n",
    "        shuffled_x1, shuffled_y1 = get_samples_safe(shuffled_x1, shuffled_y1, num_classes, subsample_rate)\n",
    "        released_mean = np.average(shuffled_x1, axis=0)\n",
    "        for ind in range(len(released_mean)):\n",
    "            c = np.random.normal(0, scale=iso_scaled[ind])\n",
    "            released_mean[ind] += c\n",
    "        avg_iso_dist_pac += np.linalg.norm(released_mean - true_mean)\n",
    "    avg_iso_dist_pac /= num_trials\n",
    "    avg_dist_pac /= num_trials\n",
    "    subsampled_dist /= num_trials\n",
    "    pac_dists[mi] = (subsampled_dist, avg_dist_pac, avg_iso_dist_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bc6b3ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007837837837837838"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29/3700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58675561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
